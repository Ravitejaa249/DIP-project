{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1IT8GbfcaUpZvBp423MsS4_-tWprNfRUC",
      "authorship_tag": "ABX9TyMLG9IsdLkPwMXGF/k7KwQe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravitejaa249/DIP-project/blob/main/Xception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# List GPUs\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"GPUs detected: {gpus}\")\n",
        "    # Enable dynamic memory allocation (optional but recommended)\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JDMoDeGz0H9",
        "outputId": "999927aa-d120-445e-f7e3-46803cebd622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "PqynUY2ny0Pe",
        "outputId": "f5de41f7-bdb5-464f-bbed-bfddca53e408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'datagen' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0ed5066a7298>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m history = model.fit(\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datagen' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# 1. GPU setup\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"GPUs detected: {gpus}\")\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n",
        "\n",
        "# 2. (Optional) Use MirroredStrategy to scale across GPUs:\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "# Configuration\n",
        "IMAGE_SIZE = (299, 299)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "def load_and_preprocess_images(directory, label, max_samples=1000):\n",
        "    # … your existing function …\n",
        "    pass\n",
        "\n",
        "# Data loading, splitting, augmentation code unchanged…\n",
        "\n",
        "with strategy.scope():\n",
        "    # Build & compile your Xception-based model here\n",
        "    input_tensor = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "    base_model = Xception(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=input_tensor,\n",
        "        pooling='avg'\n",
        "    )\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = Dense(256, activation='relu', kernel_regularizer='l2')(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=predictions)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-4),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    class_weight={0: 1, 1: np.sum(y_train == 0)/np.sum(y_train == 1)},\n",
        "    steps_per_epoch=len(X_train) // BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'\\nTest Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Unfreeze some layers for fine-tuning\n",
        "print(\"Unfreezing layers for fine-tuning...\")\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "optimizer = Adam(learning_rate=1e-5)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training\n",
        "history_fine = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "    epochs=20,  # Additional fine-tuning epochs\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    class_weight={0: 1, 1: np.sum(y_train == 0)/np.sum(y_train == 1)},\n",
        "    steps_per_epoch=len(X_train) // BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Final evaluation\n",
        "final_loss, final_acc = model.evaluate(X_test, y_test)\n",
        "print(f'\\nFinal Test Accuracy: {final_acc:.4f}')\n",
        "\n",
        "# Save model\n",
        "model.save('deepfake_detection_xception.h5')\n",
        "\n",
        "# Then your training & fine-tuning code as before…\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# 1. GPU setup\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"GPUs detected: {gpus}\")\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n",
        "\n",
        "# 2. Strategy for multi-GPU (if more than one)\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
        "\n",
        "# Configuration\n",
        "IMAGE_SIZE = (299, 299)\n",
        "BATCH_SIZE = 32\n",
        "INITIAL_EPOCHS = 10\n",
        "FINE_TUNE_EPOCHS = 20\n",
        "MAX_SAMPLES_PER_CLASS = 1000\n",
        "\n",
        "def load_and_preprocess_images(directory, label, max_samples=MAX_SAMPLES_PER_CLASS):\n",
        "    images, labels = [], []\n",
        "    filenames = os.listdir(directory)\n",
        "    np.random.shuffle(filenames)\n",
        "    for fname in filenames[:max_samples]:\n",
        "        path = os.path.join(directory, fname)\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            continue\n",
        "        img = cv2.resize(img, IMAGE_SIZE)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = img.astype('float32') / 255.0\n",
        "        images.append(img)\n",
        "        labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# 3. Load your data\n",
        "real_dir = '/content/drive/MyDrive/AI-face-detection-Dataset/real'\n",
        "fake_dir = '/content/drive/MyDrive/AI-face-detection-Dataset/AI'\n",
        "\n",
        "print(\"Loading real images...\")\n",
        "X_real, y_real = load_and_preprocess_images(real_dir, 0)\n",
        "print(\"Loading fake images...\")\n",
        "X_fake, y_fake = load_and_preprocess_images(fake_dir, 1)\n",
        "\n",
        "# Combine and split\n",
        "X = np.concatenate([X_real, X_fake], axis=0)\n",
        "y = np.concatenate([y_real, y_fake], axis=0)\n",
        "print(f\"Total samples: {len(y)},  Real: {np.sum(y==0)}, Fake: {np.sum(y==1)}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 4. Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2\n",
        ")\n",
        "\n",
        "# 5. Build & compile model under strategy.scope()\n",
        "with strategy.scope():\n",
        "    input_tensor = Input(shape=(*IMAGE_SIZE, 3))\n",
        "    base_model = Xception(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=input_tensor,\n",
        "        pooling='avg'\n",
        "    )\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = Dense(256, activation='relu', kernel_regularizer='l2')(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-4),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "# 6. Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# 7. Initial training\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "    epochs=INITIAL_EPOCHS,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    class_weight={0:1, 1: np.sum(y_train==0)/np.sum(y_train==1)},\n",
        "    steps_per_epoch=len(X_train)//BATCH_SIZE\n",
        ")\n",
        "\n",
        "# 8. Evaluate baseline\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Baseline Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "# 9. Fine-tuning: unfreeze last 20 base_model layers\n",
        "print(\"Unfreezing last 20 layers for fine-tuning…\")\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_fine = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "    epochs=FINE_TUNE_EPOCHS,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    class_weight={0:1, 1: np.sum(y_train==0)/np.sum(y_train==1)},\n",
        "    steps_per_epoch=len(X_train)//BATCH_SIZE\n",
        ")\n",
        "\n",
        "# 10. Final evaluation & save\n",
        "final_loss, final_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Fine-tuned Test Accuracy: {final_acc:.4f}\")\n",
        "\n",
        "model.save('deepfake_detection_xception.h5')\n",
        "print(\"Model saved to deepfake_detection_xception.h5\")\n"
      ],
      "metadata": {
        "id": "-kLm8-uX0u2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2EcO2El5iJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}